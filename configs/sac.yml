# Hyperparameters inspired from https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/sac.yml#L236
UnderwaterEnv:
  env_wrapper:
    - gym_underwater.env_wrappers.swim_monitor.SwimMonitor:
          allow_early_resets: True
    - gym_underwater.env_wrappers.swim_time_limit.SwimTimeLimit:
        max_episode_steps_train: 80
        max_episode_steps_inference: 100
  callback:
    - gym_underwater.callbacks.SwimEvalCallback:
          callback_after_eval:
            stable_baselines3.common.callbacks.StopTrainingOnNoModelImprovement:
              max_no_improvement_evals: 5
              min_evals: 100
              verbose: 1
          callback_on_new_best:
            stable_baselines3.common.callbacks.StopTrainingOnRewardThreshold:
              reward_threshold: 0.9999
              verbose: 1
          eval_inference_freq: 150
          eval_freq: 2
          deterministic: True
          verbose: 1
    - gym_underwater.callbacks.SwimProgressBarCallback
    - gym_underwater.callbacks.SwimCallback
  total_timesteps: !!float 1e6
  log_interval: 1
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  learning_starts: 0
  batch_size: 256
  tau: 0.02
  gamma: 0.99
  train_freq: 100
  gradient_steps: 64
  action_noise: ~
  replay_buffer_class: ~
  replay_buffer_kwargs: ~
  optimize_memory_usage: False
  ent_coef: 'auto'
  target_update_interval: 1
  target_entropy: 'auto'
  use_sde: True
  sde_sample_freq: 64
  use_sde_at_warmup: True
  stats_window_size: 3
  policy_kwargs:
    log_std_init: -2
    net_arch: [ 64, 64 ]
    use_sde: True
  verbose: 1
  device: 'auto'
  _init_setup_model: True